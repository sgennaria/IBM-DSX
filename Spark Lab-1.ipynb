{
    "cells": [
        {
            "cell_type": "markdown", 
            "source": "# Introduction to Apache Spark lab, part 1: Basic concepts\nThis notebook guides you through the basic concepts to start working with Apache Spark, including how to set up your environment, create and analyze data sets, and work with data files.\n\nThis notebook uses pySpark, the Python API for Spark. Some knowledge of Python is recommended. This notebook runs on Python 2 with Spark 2.X.\n\nIf you are new to notebooks, here's how the user interface works: [Parts of a notebook](http://datascience.ibm.com/docs/content/analyze-data/parts-of-a-notebook.html)\n", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## About Apache Spark\nApache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for processing structured data, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\n\n<img src='https://github.com/carloapp2/SparkPOT/blob/master/spark.png?raw=true' width=\"50%\" height=\"50%\"></img>\n\n\nA Spark program has a driver program and worker programs. Worker programs run on cluster nodes or in local threads. Data sets are distributed\u001d across workers. \n\n<img src='https://github.com/carloapp2/SparkPOT/blob/master/Spark%20Architecture.png?raw=true' width=\"50%\" height=\"50%\"></img>", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Table of Contents\nIn the first four sections of this notebook, you'll learn about Spark with very simple examples. In the last two sections, you'll use what you learned to analyze data files that have more realistic data sets.\n\n1. [Work with the SparkContext](#sparkcontext)<br>\n    1.1 [Invoke the SparkContext and get the version](#sparkcontext1)<br>\n2. [Work with RDDs](#rdd)<br>\n    2.1 [Create an RDD](#rdd1)<br>\n    2.2 [View the data](#rdd2)<br>\n    2.3 [Create another RDD](#rdd3)<br>\n3. [Manipulate data in RDDs](#trans)<br>\n    3.1 [Update numeric values](#trans1)<br>\n    3.2 [Add numbers in an array](#trans2)<br>\n    3.3 [Split and count strings](#trans3)<br>\n    3.4 [Count words using a Pair RDD](#trans4)<br>\n4. [Filter data](#filter)<br>\n5. [Analyze text data from a file](#wordfile)<br>\n    5.1 [Get the data from a URL](#wordfile1)<br>\n    5.2 [Create an RDD from the file](#wordfile2)<br>\n    5.3 [Filter for a word](#wordfile3)<br>\n    5.4 [Count instances of a string at the beginning of words](#wordfile4)<br>\n    5.5 [Count instances of a string within words](#wordfile5)<br>\n6. [Analyze numeric data from a file](#numfile)<br>\n7. [Summary and next steps](#summary)", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "# Lab 1 - Hello Spark\n\nThis lab will introduce you to Apache Spark.  It is written in Python and runs in IBM's Data Science Experience environment through a Jupyter notebook.  While you work, it will be valuable to reference the [Apache Spark Documentation](http://spark.apache.org/docs/latest/programming-guide.html).  Since it is Python, be careful of whitespace!", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=\"sparkcontext\"></a>\n## Step 1 - Working with the SparkContext object\n\nThe Apache Spark driver application uses the SparkContext object to allow a programming interface to interact with the driver application. The SparkContext object tells Spark how and where to access a cluster.\n\nThe Data Science Experience notebook environment predefines the Spark context for you.   This context variable will always be called 'sc'.\n\nIn other environments, you need to pick an interpreter (for example, pyspark for Python) and create a SparkConf object to initialize a SparkContext object. For example:\n<br/><br/>\n`from pyspark import SparkContext, SparkConf`<br>\n`conf = SparkConf().setAppName(appName).setMaster(master)`<br>\n`sc = SparkContext(conf=conf)`<br>\n\n<a id=\"sparkcontext1\"></a>\n### Step 1.1 - Using the spark context object, reading the <i>version</i> attribute will return the working version of Apache Spark<br><br>\n <div class=\"panel-group\" id=\"accordion-11\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-11\" href=\"#collapse1-11\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-11\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">The spark context is automatically set in a Jupyter notebook.   It is called: sc</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-11\" href=\"#collapse2-11\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-11\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type:<br>sc.version</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-11\" href=\"#collapse3-11\">\n        Optional</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-11\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Jupyter notebooks have command completion which can be invoked via the TAB key.<br>Type:<br>&nbsp;&nbsp;&nbsp;&nbsp;<i>sc.&lt;TAB&gt;</i><br>to see all the possible options within the Spark context</div>\n    </div>\n  </div>\n</div> ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#Step 1 - Check spark version\nsc.version\n", 
            "execution_count": 1, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 1, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "u'2.0.2'"
                    }
                }
            ], 
            "metadata": {
                "scrolled": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=\"rdd\"></a>\n## Step 2 - Working with Resilient Distributed Datasets (RDD)\n\nApache Spark uses an abstraction for working with data called a Resilient Distributed Dataset (RDD). An RDD is a collection of elements that can be operated on in parallel. RDDs are immutable, so you can't update the data in them. To update data in an RDD, you must create a new RDD. In Apache Spark, all work is done by creating new RDDs, transforming existing RDDs, or using RDDs to compute results. When working with RDDs, the Spark driver application automatically distributes the work across the cluster.\n\nYou can construct RDDs by parallelizing existing Python collections (lists), by manipulating RDDs, or by manipulating files in HDFS or any other storage system.\n\nYou can run these types of methods on RDDs: \n - Actions: query the data and return values\n - Transformations: manipulate data values and return pointers to new RDDs. \n\nFind more information on Python methods in the [PySpark documentation](http://spark.apache.org/docs/latest/api/python/pyspark.html).\n\n<a id=\"rdd1\"></a>\n### Step 2.1 - Create an RDD with numbers 1 to 10\n\nThere are three ways to create an RDD: parallelizing an existing collection, referencing a dataset in an external storage system which offers a Hadoop InputFormat -- or transforming an existing RDD.<br>\n<br>\nCreate an iterable or collection in your program with numbers 1 to 10 and then invoke the Spark Context's (sc) <i>parallelize()</i> method on it.<br>\n\n <div class=\"panel-group\" id=\"accordion-21\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-21\" href=\"#collapse1-21\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-21\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type:<br>\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]<br><br>\nOr we can try to be a little clever by typing:<br>\nx = range(1, 11)\n      </div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-21\" href=\"#collapse2-21\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-21\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type:<br>\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]<br>\nx_nbr_rdd = sc.parallelize(x)\n      </div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-21\" href=\"#collapse3-21\">\n        Optional Advanced</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-21\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">An optional parameter to parallelize is the number of partitions to cut the dataset into.   Spark will run one task for each partition.   Typically you want 2-4 partitions for each CPU.   Normally, Spark will set it automatically, but you can control this by specifying it manually as a second parameter to the parallelize method.<br><br>\nYou can obtain the partitions size by calling <i>&lt;RDD&gt;.getNumPartitions()</i><br>\nTry experimenting with different partitions sizes -- including ones higher than the number of values.   To see how the values are distributed use:<br><br>\n<i>\ndef f(iterator):<br>\n    &nbsp;&nbsp;&nbsp;&nbsp;\n    count = 0<br>\n    &nbsp;&nbsp;&nbsp;&nbsp;\n    for value in iterator:<br>\n    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n        count = count + 1<br>\n    &nbsp;&nbsp;&nbsp;&nbsp;\n    yield count<br>\nx_nbr_rdd.mapPartitions(f).collect()</i><br>\n      </div>\n    </div>\n  </div>\n</div> ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "x = range(1,10)\nx_nbr_rdd = sc.parallelize(x, 20)\nx_nbr_rdd.getNumPartitions()\n", 
            "execution_count": 13, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 13, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "20"
                    }
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "x_nbr_rdd = sc.parallelize(x)\nx_nbr_rdd.getNumPartitions()", 
            "execution_count": 16, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 16, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "7"
                    }
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "\ndef f(iterator):\n     count = 0\n     for value in iterator:\n         count = count + 1\n     yield count\nx_nbr_rdd.mapPartitions(f).collect()", 
            "execution_count": 17, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 17, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[1, 1, 1, 2, 1, 1, 2]"
                    }
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "Notice that there's no return value. The parallelize method didn't compute a result, which means it's a transformation. Spark only recorded how to create the RDD.\n<a id=\"rdd2\"></a>\n### Step 2.2 - View the data\nReturn the first element in the RDD<br/><br/>\n<div class=\"panel-group\" id=\"accordion-22\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-22\" href=\"#collapse1-22\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-22\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Use the <i>first()</i> method on the RDD to return the first element in an RDD.   You could also use the <i>take()</i> method with a parameter of 1.   first() and take(1) are equivalent.   Both will take the first element in the RDD's 0th partition.</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-22\" href=\"#collapse2-22\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-22\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type: <br/>\nx_nbr_rdd.first()</div>\n    </div>\n  </div>\n</div> ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "x_nbr_rdd.first()", 
            "execution_count": 19, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 19, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "1"
                    }
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "x_nbr_rdd.take(3)", 
            "execution_count": 22, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 22, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[1, 2, 3]"
                    }
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "Each number in the collection is in a different element in the RDD. Because the first() method returned a value, it is an action.\n\n### Step 2.3 - Return an array of the first five elements<br><br>\n <div class=\"panel-group\" id=\"accordion-23\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-23\" href=\"#collapse1-23\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-23\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Use the <i>take()</i> method</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-23\" href=\"#collapse2-23\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-23\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type:<br>\nx_nbr_rdd.take(5)</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-23\" href=\"#collapse3-23\">\n        Optional Advanced</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-23\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">How would you get the 5th-7th elements?   <i>take()</i> only accepts one parameter so <i>take(5,7)</i> will not work.<br>\n      </div>\n    </div>\n  </div>\n</div> \n", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#Step 2.3 - Return an array of the first five elements\nx_nbr_rdd.take(5)", 
            "execution_count": 21, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 21, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[1, 2, 3, 4, 5]"
                    }
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "test = x_nbr_rdd.take(7)\nt_rdd = sc.parallelize(test)\nt_rdd.top(3) # this is in the docs...not sure why it doesn't work", 
            "execution_count": 34, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 34, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[7, 6, 5]"
                    }
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "sc.parallelize(x_nbr_rdd.take(7)).top(3) # apparently there's no clean way to access individual elements or elements nested in the RDD.  This is pretty much how to do it.", 
            "execution_count": 35, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 35, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[7, 6, 5]"
                    }
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=\"rdd3\"></a>\n### 2.4 Create another RDD \nCreate an RDD that contains multiple strings and print the value of the first string:\n\n<div class=\"panel-group\" id=\"accordion-26\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-26\" href=\"#collapse1-26\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-26\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Create a variable with the Strings \"Hello Human\" and \"My Name is Spark\" and turn it into an RDD with the parallelize() function.   Remember that parallelize() is invoked from the Spark context!</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-26\" href=\"#collapse2-26\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-26\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type:<br>\ny = [\"Hello Human\", \"My Name is Spark\"]<br>\ny_str_rdd = sc.parallelize(y)<br>\ny_str_rdd.take(1)<br></div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-26\" href=\"#collapse3-26\">\n        Optional Advanced</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-26\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Is there a way to get the third element directly?</div>\n    </div>\n  </div>\n</div> ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# create a string array\ny = [\"Hello Human\", \"My Name is Spark\"]\n", 
            "execution_count": 44, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "Put the collection into an RDD:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# put the collection into an RDD\ny_str_rdd = sc.parallelize(y)\n\n", 
            "execution_count": 45, 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "View the first element in the RDD:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# view the first element\ny_str_rdd.take(1)", 
            "execution_count": 46, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 46, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "['Hello Human']"
                    }
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "y_str_rdd.lookup(2) # not what i was looking for...figure this out later", 
            "execution_count": 47, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 47, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[]"
                    }
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "You created the strings \"Hello Human\" and \"My Name is Spark\" and you returned \"Hello Human\" as the first element of the RDD. To analyze a set of words, you can map each word into an RDD element.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=\"trans\"></a>\n## 3. Manipulate data in RDDs\n\nRemember that to manipulate data, you use transformation functions.\n\nHere are some common Spark transformation functions that you'll be using in this notebook:\n\n - `map(func)`: returns a new RDD with the results of running the specified function on each element  \n - `filter(func)`: returns a new RDD with the elements for which the specified function returns true   \n - `distinct([numTasks]))`: returns a new RDD that contains the distinct elements of the source RDD\n - `flatMap(func)`: returns a new RDD by first running the specified function on all elements, returning 0 or more results for each original element, and then flattening the results into individual elements\n\nYou can also create functions that run a single expression and don't have a name with the Python `lambda` keyword. For example, this function returns the sum of its arguments: `lambda a , b : a + b`.\n\n<a id=\"trans1\"></a>\n### 3.1 Update numeric values\nRun the `map()` function with the `lambda` keyword to replace each element, X, in your first RDD (the one that has numeric values) with X+1. For more information go to [Transformations](http://spark.apache.org/docs/latest/programming-guide.html#transformations)\n\n<br/>\n <div class=\"panel-group\" id=\"accordion-24\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-24\" href=\"#collapse1-24\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-24\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Use the <i>map(func)</i> function on the RDD.   Map invokes function <i>func</i> on each element of the RDD.   You can also use a inline (or lambda) function.   The syntax for a lambda function is:<br>\n\nlambda &lt;var&gt;: &lt;myCode&gt;\n</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-24\" href=\"#collapse2-24\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-24\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type:<br>\nx_nbr_rdd_2 = x_nbr_rdd.map(lambda x: x+1)</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-24\" href=\"#collapse3-24\">\n        Optional Advanced</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-24\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Instead of using lambda, write a python function which increments the value by 1 and pass that function to map()</div>\n    </div>\n  </div>\n</div> \n", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# Step 3.1 - Write your map function\nx_nbr_rdd_2 = x_nbr_rdd.map(lambda x: x+1)\nx_nbr_rdd_2.collect()\n\n", 
            "execution_count": 58, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 58, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[2, 3, 4, 5, 6, 7, 8, 9, 10]"
                    }
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# Optional Advanced\ndef incr(x):\n    return x+1\nx_nbr_rdd_3 = x_nbr_rdd.map(incr) # not sure how this works\nx_nbr_rdd_3.collect()", 
            "execution_count": 100, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 100, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
                    }
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "Note that there was no result for Step 3.1.  Why was this?  Use collect() to look at all the elements of the new RDD.<br>\nType:<br>\n&nbsp;&nbsp;&nbsp;&nbsp; x_nbr_rdd_2.collect()   ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# Check out the elements of the new RDD. Warning: Be careful with this in real life! Collect returns everything!  Returning a large data set might be not be very useful. No-one wants to scroll through a million rows!\nx_nbr_rdd_2.collect()", 
            "execution_count": 57, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 57, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[2, 3, 4, 5, 6, 7, 8, 9, 10]"
                    }
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=\"trans2\"></a>\n### 3.2 Sum the numbers in an RDD\nCreate an RDD consisting of a collection of numbers. \n\n<div class=\"panel-group\" id=\"accordion-32\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-32\" href=\"#collapse1-32\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-32\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type:<br>\n\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]<br><br>\nOr we can try to be a little clever by typing:<br>\n\nx = range(1, 11)\n      </div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-32\" href=\"#collapse2-32\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-32\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type:<br>\n\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]<br>\nx_nbr_rdd = sc.parallelize(x)\n      </div>\n    </div>\n  </div>", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "source": "x = range(1,11)\nx_nbr_rdd = sc.parallelize(x)", 
            "execution_count": 76, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "Calculate the Sum\n\n<div class=\"panel-group\" id=\"accordion-21\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion2-32\" href=\"#collapse3-32\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-32\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type:<br>\n&nbsp;&nbsp;&nbsp;&nbsp;\nUse the reduce action which aggregates the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.\n      </div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion2-32\" href=\"#collapse4-32\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse4-32\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type:<br>\nx_nbr_rdd.reduce(lambda x,y: x+y) <br>\n      </div>\n    </div>\n  </div>", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "x_nbr_rdd.reduce(lambda x,y: x+y)", 
            "execution_count": 77, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 77, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "55"
                    }
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=\"trans3\"></a>\n### 3.3 Split and count text strings\n\nCreate an RDD with the following text strings and show the first element:<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;\"IBM Data Science Experience is built for enterprise-scale deployment.\"<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;\"Manage your data, your analytical assets, and your projects in a secured cloud environment.\"<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;\"When you create an account in the IBM Data Science Experience, we deploy for you a Spark as a Service instance to power your analysis and 5 GB of IBM Object Storage to store your data.\"<br/><br/>\n <div class=\"panel-group\" id=\"accordion-27\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-27\" href=\"#collapse1-27\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-27\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Use an array -- [] -- to contain all three strings.   Don't forget to enclose them in quotes!</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-27\" href=\"#collapse2-27\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-27\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">z = [ \"IBM Data Science Experience is built for enterprise-scale deployment.\", \"Manage your data, your analytical assets, and your projects in a secured cloud environment.\", \"When you create an account in the IBM Data Science Experience, we deploy for you a Spark as a Service instance to power your analysis and 5 GB of IBM Object Storage to store your data.\" ]<br/>\nz_str_rdd = sc.parallelize(z)<br/>\nz_str_rdd.first()      \n      </div>\n    </div>\n  </div>\n</div> ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# create and parallelize strings\ns = [\"IBM Data Science Experience is built for enterprise-scale deployment.\",\"Manage your data, your analytical assets, and your projects in a secured cloud environment.\",\"When you create an account in the IBM Data Science Experience, we deploy for you a Spark as a Service instance to power your analysis and 5 GB of IBM Object Storage to store your data.\"]\nz_str_rdd = sc.parallelize(s)\nz_str_rdd.first()", 
            "execution_count": 79, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 79, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "'IBM Data Science Experience is built for enterprise-scale deployment.'"
                    }
                }
            ], 
            "metadata": {
                "scrolled": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "# Step 3.4 - Split all the entries in the RDD on the spaces.  Then print it out.  Pay careful attention to the new format.\n<br/>\n <div class=\"panel-group\" id=\"accordion-210\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-210\" href=\"#collapse1-210\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-210\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">To split on spaces, use the <a href=\"https://docs.python.org/2/library/stdtypes.html#string-methods\"><i>split()</i></a> function.</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-210\" href=\"#collapse2-210\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-210\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Since you want to run on every line, use <i>map()</i> on the RDD and write a lambda function to call <i>split()</i></div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-210\" href=\"#collapse3-210\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-210\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type: <br>\nz_str_rdd_split = z_str_rdd.map(lambda line: line.split(\" \"))<br/>\nz_str_rdd_split.collect()<br><br>\nQuestion: Is there any difference between split(\" \") and split()?</div>\n    </div>\n  </div>\n</div> ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#Step 2.10 - Perform a map transformation to split all entries in the RDD\n#Check out the entries in the new RDD\nz_str_rdd_split = z_str_rdd.map(lambda x: x.split(\" \"))\nz_str_rdd_split.collect()", 
            "execution_count": 84, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 84, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[['IBM',\n  'Data',\n  'Science',\n  'Experience',\n  'is',\n  'built',\n  'for',\n  'enterprise-scale',\n  'deployment.'],\n ['Manage',\n  'your',\n  'data,',\n  'your',\n  'analytical',\n  'assets,',\n  'and',\n  'your',\n  'projects',\n  'in',\n  'a',\n  'secured',\n  'cloud',\n  'environment.'],\n ['When',\n  'you',\n  'create',\n  'an',\n  'account',\n  'in',\n  'the',\n  'IBM',\n  'Data',\n  'Science',\n  'Experience,',\n  'we',\n  'deploy',\n  'for',\n  'you',\n  'a',\n  'Spark',\n  'as',\n  'a',\n  'Service',\n  'instance',\n  'to',\n  'power',\n  'your',\n  'analysis',\n  'and',\n  '5',\n  'GB',\n  'of',\n  'IBM',\n  'Object',\n  'Storage',\n  'to',\n  'store',\n  'your',\n  'data.']]"
                    }
                }
            ], 
            "metadata": {
                "scrolled": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Step 3.5 - Explore a new transformation: <a href=\"https://spark.apache.org/docs/1.6.0/api/python/pyspark#pyspark.RDD.flatMap\">flatMap</a>\n<br/>\nWe want to count the words in <b>all</b> the lines, but currently they are split by line.   We need to 'flatten' the line return values into one object.<br/>\nflatMap will \"flatten\" all the elements of an RDD element into 0 or more output terms.<br/><br/>\n <div class=\"panel-group\" id=\"accordion-211\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-211\" href=\"#collapse1-211\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-211\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\"><i>flatmap()</i> parameters work the same way as in <i>map()</i></div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-211\" href=\"#collapse2-211\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-211\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type:<br/>\nz_str_rdd_split_flatmap = z_str_rdd.flatMap(lambda line: line.split())<br/>\nprint(z_str_rdd_split_flatmap.collect())<br/></div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-211\" href=\"#collapse3-211\">\n        Optional Advanced</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-211\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Use the replace() and lower() methods to remove all commas and periods then make everything lower-case</div>\n    </div>\n  </div>\n</div> ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# Step 3.5 - Learn the difference between two transformations: map and flatMap.\nz_str_rdd_split_flatmap = z_str_rdd.flatMap(lambda line: line.split()) # same as split(\" \")\nprint(z_str_rdd_split_flatmap.collect())\n# print the result so that if you do the advanced section both results will be output\n\n", 
            "execution_count": 87, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "['IBM', 'Data', 'Science', 'Experience', 'is', 'built', 'for', 'enterprise-scale', 'deployment.', 'Manage', 'your', 'data,', 'your', 'analytical', 'assets,', 'and', 'your', 'projects', 'in', 'a', 'secured', 'cloud', 'environment.', 'When', 'you', 'create', 'an', 'account', 'in', 'the', 'IBM', 'Data', 'Science', 'Experience,', 'we', 'deploy', 'for', 'you', 'a', 'Spark', 'as', 'a', 'Service', 'instance', 'to', 'power', 'your', 'analysis', 'and', '5', 'GB', 'of', 'IBM', 'Object', 'Storage', 'to', 'store', 'your', 'data.']\n", 
                    "name": "stdout"
                }
            ], 
            "metadata": {
                "scrolled": false
            }
        }, 
        {
            "cell_type": "code", 
            "source": "# Optional Advanced\n# What do you notice? How are the outputs of 3.4 and 3.5 different?\nz_str_rdd_split_flatmap_2 = z_str_rdd_split_flatmap.map(lambda x: x.lower().replace(',',''))\nprint(z_str_rdd_split_flatmap_2.collect())\n", 
            "execution_count": 96, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "['ibm', 'data', 'science', 'experience', 'is', 'built', 'for', 'enterprise-scale', 'deployment.', 'manage', 'your', 'data', 'your', 'analytical', 'assets', 'and', 'your', 'projects', 'in', 'a', 'secured', 'cloud', 'environment.', 'when', 'you', 'create', 'an', 'account', 'in', 'the', 'ibm', 'data', 'science', 'experience', 'we', 'deploy', 'for', 'you', 'a', 'spark', 'as', 'a', 'service', 'instance', 'to', 'power', 'your', 'analysis', 'and', '5', 'gb', 'of', 'ibm', 'object', 'storage', 'to', 'store', 'your', 'data.']\n", 
                    "name": "stdout"
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Step 3.6 - Augment each entry in the previous RDD with the number \"1\" to create pairs or tuples. The first element of the tuple will be the word and the second elements of the tuple will be the digit \"1\".  This is a common step in performing a count as we need values to sum.\n<br>\n <div class=\"panel-group\" id=\"accordion-212\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-212\" href=\"#collapse1-212\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-212\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Maps don't always have to perform calculations, they can just echo values as well.   Simply echo the value and a 1<br></div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-212\" href=\"#collapse2-212\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-212\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">We need to create tuples which are values enclosed in parenthesis, so you'll need to enclose the value, 1 in parens.   For example: (x, 1)<br></div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-212\" href=\"#collapse3-212\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-212\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type:<br>\ncountWords = z_str_rdd_split_flatmap.map(lambda word:(word,1))<br>\ncountWords.collect()<br></div>\n    </div>\n  </div>\n</div> ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#Step 3.6 - Create pairs or tuple RDD and print it.\ncountWords = z_str_rdd_split_flatmap_2.map(lambda x: (x,1))\ncountWords.collect()", 
            "execution_count": 98, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 98, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('ibm', 1),\n ('data', 1),\n ('science', 1),\n ('experience', 1),\n ('is', 1),\n ('built', 1),\n ('for', 1),\n ('enterprise-scale', 1),\n ('deployment.', 1),\n ('manage', 1),\n ('your', 1),\n ('data', 1),\n ('your', 1),\n ('analytical', 1),\n ('assets', 1),\n ('and', 1),\n ('your', 1),\n ('projects', 1),\n ('in', 1),\n ('a', 1),\n ('secured', 1),\n ('cloud', 1),\n ('environment.', 1),\n ('when', 1),\n ('you', 1),\n ('create', 1),\n ('an', 1),\n ('account', 1),\n ('in', 1),\n ('the', 1),\n ('ibm', 1),\n ('data', 1),\n ('science', 1),\n ('experience', 1),\n ('we', 1),\n ('deploy', 1),\n ('for', 1),\n ('you', 1),\n ('a', 1),\n ('spark', 1),\n ('as', 1),\n ('a', 1),\n ('service', 1),\n ('instance', 1),\n ('to', 1),\n ('power', 1),\n ('your', 1),\n ('analysis', 1),\n ('and', 1),\n ('5', 1),\n ('gb', 1),\n ('of', 1),\n ('ibm', 1),\n ('object', 1),\n ('storage', 1),\n ('to', 1),\n ('store', 1),\n ('your', 1),\n ('data.', 1)]"
                    }
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=\"trans4\"></a>\n### Step 3.7 Now we have above what is known as a [Pair RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions). Each entry in the RDD has a KEY and a VALUE.<br>\nThe KEY is the word (Light, of, the, ...) and the value is the number \"1\".  \nWe can now AGGREGATE this RDD by summing up all the values BY KEY<br><br>\n <div class=\"panel-group\" id=\"accordion-213\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-213\" href=\"#collapse1-213\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-213\" class=\"panel-collapse collapse in\">\n      <div class=\"panel-body\">We want to sum all values by key in the key-value pairs.  The generic function to do this is <i>reduceByKey(func)</i>:<br>\n      &nbsp;&nbsp;&nbsp;&nbsp;When called on a dataset of (K [Key], V [Value]) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V.<br><br>Which means func(v1, v2) runs across all values for a specific key.  Think of v1 as the output (initialized as 0 or \"\") and v2 as the iterated value over each value in the set with the same key.  With each iterated value, v1 is updated.<br>\n      Use a lambda function to sum up the values just as you wrote for <i>map()</i></div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-213\" href=\"#collapse2-213\">\n         Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-213\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type:<br>\ncountWords2 = countWords.reduceByKey(lambda x,y: x+y)<br>\ncountWords2.collect()<br></div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-213\" href=\"#collapse3-213\">\n        Optional Advanced</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-213\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Sort the results by the count.   You could call <i>sortBy()</i> on the result....<br>\n      Also, while the function used in <i>map()</i> has only one parameter, when working with Pair RDDs, that parameter is an array of two values....\n      </div>\n    </div>\n  </div>\n</div> \n", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# Step 3.7 - Check out the results of the aggregation\ncountWords2 = countWords.reduceByKey(lambda x,y:x+y)\ncountWords2.collect()\n\n", 
            "execution_count": 103, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 103, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('enterprise-scale', 1),\n ('for', 2),\n ('storage', 1),\n ('secured', 1),\n ('when', 1),\n ('as', 1),\n ('spark', 1),\n ('cloud', 1),\n ('instance', 1),\n ('ibm', 3),\n ('built', 1),\n ('and', 2),\n ('a', 3),\n ('we', 1),\n ('power', 1),\n ('service', 1),\n ('account', 1),\n ('analysis', 1),\n ('deployment.', 1),\n ('5', 1),\n ('experience', 2),\n ('analytical', 1),\n ('the', 1),\n ('projects', 1),\n ('store', 1),\n ('data.', 1),\n ('deploy', 1),\n ('science', 2),\n ('manage', 1),\n ('an', 1),\n ('environment.', 1),\n ('gb', 1),\n ('is', 1),\n ('data', 3),\n ('create', 1),\n ('you', 2),\n ('assets', 1),\n ('of', 1),\n ('object', 1),\n ('to', 2),\n ('in', 2),\n ('your', 5)]"
                    }
                }
            ], 
            "metadata": {
                "scrolled": true
            }
        }, 
        {
            "cell_type": "code", 
            "source": "# optional advanced\ncountWords2.sortBy(lambda x: x[0]).collect()", 
            "execution_count": 107, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 107, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('5', 1),\n ('a', 3),\n ('account', 1),\n ('an', 1),\n ('analysis', 1),\n ('analytical', 1),\n ('and', 2),\n ('as', 1),\n ('assets', 1),\n ('built', 1),\n ('cloud', 1),\n ('create', 1),\n ('data', 3),\n ('data.', 1),\n ('deploy', 1),\n ('deployment.', 1),\n ('enterprise-scale', 1),\n ('environment.', 1),\n ('experience', 2),\n ('for', 2),\n ('gb', 1),\n ('ibm', 3),\n ('in', 2),\n ('instance', 1),\n ('is', 1),\n ('manage', 1),\n ('object', 1),\n ('of', 1),\n ('power', 1),\n ('projects', 1),\n ('science', 2),\n ('secured', 1),\n ('service', 1),\n ('spark', 1),\n ('storage', 1),\n ('store', 1),\n ('the', 1),\n ('to', 2),\n ('we', 1),\n ('when', 1),\n ('you', 2),\n ('your', 5)]"
                    }
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=\"filter\"></a>\n## 4. Filter data\n\nThe filter command creates a new RDD from another RDD based on a filter criteria.\nThe filter syntax is: \n\n`.filter(lambda line: \"Filter Criteria\")`\n\nHint - The criteria for a simple string check is: &#60;string&#62; in &#60;variable&#62;.\n\nFind the number of instances of the word `IBM` in the `z_str_rdd_split_flatmap` RDD:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "words_rd3 = z_str_rdd_split_flatmap.filter(lambda line: \"IBM\" in line) \n\nprint \"The count of words \" + str(words_rd3.first())\nprint \"Is: \" + str(words_rd3.count())", 
            "execution_count": 110, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "The count of words IBM\nIs: 3\n", 
                    "name": "stdout"
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=\"wordfile\"></a>\n## 5. Analyze text data from a file\nIn this section, you'll download a file from a URL, create an RDD from it, and analyze the text in it.\n\n<a id=\"wordfile1\"></a>\n### Step 5.1 - Read the Apache Spark README.md file from Github.  The ! allows you to embed file system commands\n<br/>\nWe remove README.md in case there was an updated version -- but also for another reason you will discover in Lab 2<br/><br/>\nType:<br/>\n\n&nbsp;&nbsp;&nbsp;&nbsp;!rm README.md* -f<br>\n&nbsp;&nbsp;&nbsp;&nbsp;!wget https://raw.githubusercontent.com/apache/spark/master/README.md<br>\n", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# Step 5.1 - Pull data file into workbench\n!rm README.md* -f\n!wget https://raw.githubusercontent.com/apache/spark/master/README.md", 
            "execution_count": 111, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "--2017-09-19 11:27:13--  https://raw.githubusercontent.com/apache/spark/master/README.md\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3809 (3.7K) [text/plain]\nSaving to: \u2018README.md\u2019\n\n100%[======================================>] 3,809       --.-K/s   in 0s      \n\n2017-09-19 11:27:13 (18.5 MB/s) - \u2018README.md\u2019 saved [3809/3809]\n\n", 
                    "name": "stdout"
                }
            ], 
            "metadata": {
                "scrolled": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=\"wordfile1\"></a>\n### Step 5.2 - Create an RDD by reading from the local filesystem and count the number of lines  Here is the [textfile()](http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=textfile#pyspark.SparkContext.textFile) documentation.<br><br>\n <div class=\"panel-group\" id=\"accordion-52\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-52\" href=\"#collapse1-52\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-52\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">README.md has been loaded into local storage so there is no path needed.   <i>textFile()</i> returns an RDD -- you do not have to parallelize the result.</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-52\" href=\"#collapse2-52\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-52\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type:<br>\ntextfile_rdd = sc.textFile(\"README.md\")<br>\ntextfile_rdd.count()<br></div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-52\" href=\"#collapse3-52\">\n        Optional Advanced</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-52\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">By default, <i>textFile()</i> uses UTF-8 format.   Read the file as UNICODE (refer to the docs).</div>\n    </div>\n  </div>\n</div> \n", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# Step 5.2 - Create RDD from data file\ntextfile_rdd = sc.textFile(\"README.md\", use_unicode=False)\nprint(textfile_rdd.count())\n\n# Optional Advanced\ntextfile_rdd_uni = sc.textFile(\"README.md\", use_unicode=True)\nprint(textfile_rdd_uni.count())", 
            "execution_count": 117, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "103\n103\n", 
                    "name": "stdout"
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=\"wordfile3\"></a>\n### Step 5.3 - Use [filter](http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=filter#pyspark.RDD.filter) transformation to include lines that contain \"Spark\". Python allows us to use the 'in' syntax to search strings.<br>\nWe will also take a look at the first line in the newly filtered RDD. <br><br>\n <div class=\"panel-group\" id=\"accordion-33\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-33\" href=\"#collapse1-33\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-33\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\"><i>filter()</i>, just like <i>map()</i> can take a lambda function as its input</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-33\" href=\"#collapse2-33\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-33\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type:<br>\nSpark_lines = textfile_rdd.filter(lambda line: \"Spark\" in line)<br>\nSpark_lines.first()<br></div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-33\" href=\"#collapse3-33\">\n        Advanced Optional</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-33\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">There are 19 lines which contain the word \"Spark\".   Find all lines which contain it when case-insensitive<br></div>\n    </div>\n  </div>\n</div> ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "#Step 5.3 - Filter for only lines with word Spark\nSpark_lines = textfile_rdd.filter(lambda line: \"Spark\" in line)\nprint(Spark_lines.first())\nprint(Spark_lines.collect())\n\n# Advanced optional\n", 
            "execution_count": 122, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "# Apache Spark\n['# Apache Spark', 'Spark is a fast and general cluster computing system for Big Data. It provides', 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,', 'and Spark Streaming for stream processing.', 'You can find the latest Spark documentation, including a programming', '## Building Spark', 'Spark is built using [Apache Maven](http://maven.apache.org/).', 'To build Spark and its example programs, run:', 'You can build Spark using more than one thread by using the -T option with Maven, see [\"Parallel builds in Maven 3\"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).', '[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 'For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](http://spark.apache.org/developer-tools.html).', 'The easiest way to start using Spark is through the Scala shell:', 'Spark also comes with several sample programs in the `examples` directory.', '    ./bin/run-example SparkPi', '    MASTER=spark://host:7077 ./bin/run-example SparkPi', 'Testing first requires [building Spark](#building-spark). Once Spark is built, tests', 'Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported', 'Hadoop, you must build Spark against the same version that your cluster runs.', 'in the online documentation for an overview on how to configure Spark.', 'Please review the [Contribution to Spark guide](http://spark.apache.org/contributing.html)']\n", 
                    "name": "stdout"
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=\"wordfile4\"></a>\n### Step 5.4 - Print the number of Spark lines in this filtered RDD out of the total number and print the result as a concatenated string.<br/><br/>\n <div class=\"panel-group\" id=\"accordion-34\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-34\" href=\"#collapse1-34\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-34\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">The <i>print()</i> statement prints to the console.  (Note: be careful on a cluster because a print on a distributed machine will not be seen).  You can cast integers to string by using the <i>str()</i> method.</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-34\" href=\"#collapse2-34\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-34\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Strings can be concatenated together with the + sign.   You can mark a statement as spanning multiple lines by putting a \\ at the end of the line.</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-34\" href=\"#collapse3-34\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-34\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type:<br>\nprint \"The file README.md has \" + str(Spark_lines.count()) + \\<br/>\n\" of \" + str(textfile_rdd.count()) + \\<br/>\n\" lines with the word Spark in it.\"<br/></div>\n    </div>\n  </div>\n</div> ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# Step 5.4 - count the number of lines\nprint \"The file README.md has \" + str(Spark_lines.count()) + \\\n\" of \" + str(textfile_rdd.count()) + \\\n\" lines with the word Spark in it.\"", 
            "execution_count": 123, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "The file README.md has 20 of 103 lines with the word Spark in it.\n", 
                    "name": "stdout"
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=\"wordfile5\"></a>\n### Step 5.5 - Now count the number of times the word Spark appears in the original text, not just the number of lines that contain it.\n <div class=\"panel-group\" id=\"accordion-35\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-35\" href=\"#collapse1-35\">\n        Hint</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-35\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">\n        Looking back at previous exercises, you will need to: <br>\n        &nbsp;&nbsp;&nbsp;&nbsp;1 - Execute a flatMap transformation on the original RDD Spark_lines and split on white space.<br>\n        &nbsp;&nbsp;&nbsp;&nbsp;2 - Use filter to include all instances of the word Spark<br>\n        &nbsp;&nbsp;&nbsp;&nbsp;3 - Count all instances<br>\n        &nbsp;&nbsp;&nbsp;&nbsp;4 - Print the total count<br><br>\n      </div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-35\" href=\"#collapse2-35\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-35\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">\n      Flattened_Spark_lines = Spark_lines.flatMap(lambda line: line.split())<br>\n      Spark_instances = Flattened_Spark_lines.filter(lambda word: \"Spark\" in word)<br>\n      print \"Number of Spark instances: \",str(Spark_instances.count())\n      </div>\n      </div>\n    </div>\n    <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-35\" href=\"#collapse3-35\">\n        Optional Advanced</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-35\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Put the entire statement on one line and make the filter case-insensitive.</div>\n    </div>\n  </div>\n</div> ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# Step 5.5\nFlattened_Spark_lines = Spark_lines.flatMap(lambda line: line.split())\nSpark_instances = Flattened_Spark_lines.filter(lambda word: \"Spark\" in word)\nprint \"Number of Spark instances: \",str(Spark_instances.count()) \n#Optional Advanced\n", 
            "execution_count": 124, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Number of Spark instances:  21\n", 
                    "name": "stdout"
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "print \"Number of Spark instances: \",str(Spark_lines.flatMap(lambda line: line.split()).filter(lambda word: \"spark\" in word.lower()).count()) ", 
            "execution_count": 139, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Number of Spark instances:  24\n", 
                    "name": "stdout"
                }
            ], 
            "metadata": {
                "scrolled": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=\"numfile\"></a>\n## Step 6 - Perform analysis on a data file\nThis part is a little more open ended and there are a few ways to complete it.  Scroll up to previous examples for some guidance.  You will download a data file, transform the data, and then average the prices.  The data file will be a sample of tech stock prices over six days. <br>\n\nData Location: https://raw.githubusercontent.com/JosephKambourakisIBM/SparkPoT/master/StockPrices.csv<br>\nThe data file is a csv<br/><br/>\nHere is a sample of the file:<br/>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;IBM,159.720001,159.399994,158.880005,159.539993,159.550003,160.350006\n\nWe leverage map-reduce to create a generic solution but there are multiple ways to solve this problem.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "# Step 6 - Delete the file if it exists, download a new copy and load it into an RDD\n!rm StockPrices.csv -f\n!wget https://raw.githubusercontent.com/JosephKambourakisIBM/SparkPoT/master/StockPrices.csv\n    \nSP = sc.textFile(\"StockPrices.csv\")\nSP.collect()", 
            "execution_count": 140, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "--2017-09-19 11:48:52--  https://raw.githubusercontent.com/JosephKambourakisIBM/SparkPoT/master/StockPrices.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 244 [text/plain]\nSaving to: \u2018StockPrices.csv\u2019\n\n100%[======================================>] 244         --.-K/s   in 0s      \n\n2017-09-19 11:48:52 (34.6 MB/s) - \u2018StockPrices.csv\u2019 saved [244/244]\n\n", 
                    "name": "stdout"
                }, 
                {
                    "output_type": "execute_result", 
                    "execution_count": 140, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[u'IBM,159.720001,159.399994,158.880005,159.539993,159.550003,160.350006',\n u'MSFT,58.099998,57.889999,57.459999,57.59,57.669998,57.610001',\n u'AAPL,106.82,106,106.099998,106.730003,107.730003,107.699997',\n u'ORCL,41.310001,41.310001,41.220001,41.16,41.25,41.25']"
                    }
                }
            ], 
            "metadata": {
                "scrolled": true
            }
        }, 
        {
            "cell_type": "code", 
            "source": "test1 = SP.map(lambda line: line.split(','))\ntest1.collect()", 
            "execution_count": 142, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 142, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[[u'IBM',\n  u'159.720001',\n  u'159.399994',\n  u'158.880005',\n  u'159.539993',\n  u'159.550003',\n  u'160.350006'],\n [u'MSFT',\n  u'58.099998',\n  u'57.889999',\n  u'57.459999',\n  u'57.59',\n  u'57.669998',\n  u'57.610001'],\n [u'AAPL',\n  u'106.82',\n  u'106',\n  u'106.099998',\n  u'106.730003',\n  u'107.730003',\n  u'107.699997'],\n [u'ORCL',\n  u'41.310001',\n  u'41.310001',\n  u'41.220001',\n  u'41.16',\n  u'41.25',\n  u'41.25']]"
                    }
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "test2 = test1.flatMap(lambda row: map(lambda x: (row[0],[float(row[x]), 1]), range(1,len(row))))\ntest2.collect()", 
            "execution_count": 144, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 144, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[(u'IBM', [159.720001, 1]),\n (u'IBM', [159.399994, 1]),\n (u'IBM', [158.880005, 1]),\n (u'IBM', [159.539993, 1]),\n (u'IBM', [159.550003, 1]),\n (u'IBM', [160.350006, 1]),\n (u'MSFT', [58.099998, 1]),\n (u'MSFT', [57.889999, 1]),\n (u'MSFT', [57.459999, 1]),\n (u'MSFT', [57.59, 1]),\n (u'MSFT', [57.669998, 1]),\n (u'MSFT', [57.610001, 1]),\n (u'AAPL', [106.82, 1]),\n (u'AAPL', [106.0, 1]),\n (u'AAPL', [106.099998, 1]),\n (u'AAPL', [106.730003, 1]),\n (u'AAPL', [107.730003, 1]),\n (u'AAPL', [107.699997, 1]),\n (u'ORCL', [41.310001, 1]),\n (u'ORCL', [41.310001, 1]),\n (u'ORCL', [41.220001, 1]),\n (u'ORCL', [41.16, 1]),\n (u'ORCL', [41.25, 1]),\n (u'ORCL', [41.25, 1])]"
                    }
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "test3 = test2.reduceByKey(lambda x,y: [x[0] + y[0], x[1] + y[1]])\ntest3.collect()", 
            "execution_count": 146, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 146, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[(u'AAPL', [641.080001, 6]),\n (u'ORCL', [247.500003, 6]),\n (u'MSFT', [346.319995, 6]),\n (u'IBM', [957.4400019999999, 6])]"
                    }
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "test4 = test3.map(lambda x: (x[0], float(x[1][0]) / int(x[1][1])))\ntest4.collect()", 
            "execution_count": 147, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 147, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[(u'AAPL', 106.84666683333334),\n (u'ORCL', 41.2500005),\n (u'MSFT', 57.71999916666667),\n (u'IBM', 159.57333366666666)]"
                    }
                }
            ], 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "<a id=\"summary\"></a>\n## 7. Summary and next steps\n\nYou've learned how to work with data in RDDs to discover useful information.\n\nDig deeper:\n - [Apache Spark documentation](http://spark.apache.org/documentation.html)\n - [PySpark documentation](http://spark.apache.org/docs/latest/api/python/pyspark.html)", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Authors\nCarlo Appugliese is a Spark and Hadoop evangelist at IBM.<br/>\nBraden Callahan is a Big Data Technical Specialist for IBM.<br/>\nRoss Lewis is a Big Data Technical Sales Specialist for IBM.<br/>\nMokhtar Kandil is a World Wide Big Data Technical Specialist for IBM.<br/>\nJoel Patterson is a Big Data Technical Specialist for IBM", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "source": "", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }
    ], 
    "nbformat": 4, 
    "nbformat_minor": 1, 
    "metadata": {
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 2 with Spark 2.0", 
            "name": "python2-spark20"
        }, 
        "language_info": {
            "pygments_lexer": "ipython2", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "version": "2.7.11", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "name": "python"
        }
    }
}